import math
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

from camera_operations import extract_origin_and_direction
from dataset import *
from config import *




class CoarseNeRFSampler:
    """
    @brief Class to sample points along the ray and make the input tensor for the coarse model

    The samples are taken randomly (uniform) between uniform points along the ray between t_min and t_max. The number of samples is given by Nc.
    """
    def __init__(self, t_min=0, t_max=8, Nc=64):
        self.t_min = t_min
        self.t_max = t_max
        self.Nc = Nc
    
    def sample(self, o, d):
        """
        @brief Sample points along the ray
        @param o: Tensor of shape (..., 3) containing the origin of the rays
        @param d: Tensor of shape (..., 3) containing the direction of the rays
        """
        delta = (self.t_max - self.t_min) / self.Nc
        ts = torch.linspace(self.t_min, self.t_max, self.Nc+1, device = device, dtype = dtype) + \
             torch.rand(list(o.shape[:-1]) + [self.Nc+1], device = device, dtype = dtype) * delta
        return ts

    def make_x(self, o, d):
        """
        Make the input tensor for the coarse model
        @param o: Tensor of shape (..., 3) containing the origin of the ray
        @param d: Tensor of shape (..., 3) containing the direction of the ray
        """
        ts = self.sample(o, d)
        return torch.cat([o[..., None, :] + d[..., None, :] * ts[..., :, None], ts[..., :, None], d[..., None, :].expand_as(ts[..., :, None])], dim=-1)
    

def inverse_transform_sampling(cdf, Nf):
    """
    @param cdf: Tensor of shape (n_rays, Nc) containing the cumulative distribution function
    @param Nf: Number of samples to generate
    @output: Tensor of shape (n_rays, Nf) containing the samples, generated by inverse transform sampling with interpolation
    """
    cdf_shape = cdf.shape
    cdf_flat = cdf.reshape(-1, cdf_shape[-1])
    n_rays = cdf_flat.shape[0]
    Nc = cdf_shape[-1]

    u = torch.rand(n_rays, Nf, device=device, dtype=dtype)
    indices = torch.searchsorted(cdf, u, right=True)
    indices = torch.clamp(indices, 0, Nc-1)
    cdf = torch.cat([torch.zeros(n_rays, 1, device=device, dtype=dtype), cdf], dim=1)
    u = (u - cdf[torch.arange(n_rays)[:, None], indices]) / (cdf[torch.arange(n_rays)[:, None], indices+1] - cdf[torch.arange(n_rays)[:, None], indices])
    ts = (indices + u) / Nc
    return ts.reshape(*cdf_shape[:-1], Nf)

class FineNeRFSampler:
    """
    Uses a NeRF model to sample points along the ray
    """
    def __init__(self, model, Nf=128, Nc=64, t_min=0, t_max=8):
        self.model = model
        self.sampler = CoarseNeRFSampler(t_min=t_min, t_max=t_max, Nc=Nc)
        self.Nf = Nf
        self.t_min = t_min
        self.t_max = t_max

    # def sample(self, o, d):
    #     """
    #     @param o: Tensor of shape (n_rays, 3) containing the origin of the ray
    #     @param d: Tensor of shape (n_rays, 3) containing the direction of the ray
    #     @output: Tensor of shape (n_rays, Nf) containing the samples
    #     """
    #     t = self.sampler.sample(o, d)
    #     sigma = self.model.sigma_from_ray(o, d, t)

    #     delta = t[:, 1:] - t[:, :-1]
    #     Ti = torch.exp(- torch.cumsum(sigma * delta, dim=1))

    #     wi = (1 - torch.exp(- sigma * delta)) * Ti
    #     pdf = wi / wi.sum(dim=1, keepdim=True)
    #     cdf = torch.cumsum(pdf, dim=1)
               
    #     # Inverse pdf sampling
    #     ts = inverse_transform_sampling(cdf, self.Nf)
    #     # ts = torch.cat([ts, t], dim=1)

    #     return ts

    def sample(self, o, d):
        """
        @param o: Tensor of shape (..., 3) containing the origin of the ray
        @param d: Tensor of shape (..., 3) containing the direction of the ray
        @output: Tensor of shape (..., Nf) containing the samples
        """
        t = self.sampler.sample(o, d)
        reshape = False
        if o.ndim == 1:
            reshape = True
            o = o.unsqueeze(0)
            d = d.unsqueeze(0)
            t = t.unsqueeze(0)

        sigma = self.model.sigma_from_ray(o, d, t)

        delta = t[..., 1:] - t[..., :-1]
        Ti = torch.exp(- torch.cumsum(sigma * delta, dim=1))

        wi = (1 - torch.exp(- sigma * delta)) * Ti
        pdf = wi / wi.sum(dim=1, keepdim=True)
        cdf = torch.cumsum(pdf, dim=1)
        # return cdf
               
        # Inverse pdf sampling
        ts = inverse_transform_sampling(cdf, self.Nf) * (self.t_max - self.t_min) + self.t_min

        ts = concatenated = torch.cat((ts, t), dim=1)
        ts, _ = torch.sort(concatenated, dim=1)


        return ts if not reshape else ts[0]

class NeRF(nn.Module):
    def __init__(self, Lx=6 , Ld =4 , lr =1e-4 , epochs = 1000, hidden_dim=64, sampler=None, name="model", Nc=64):
        super(NeRF, self).__init__()
        self.Lx = Lx #positional encoding for x
        self.Ld = Ld #positional encoding for d
        self.lr = lr
        self.Nc = Nc # Number of samples along the ray
        self.epochs = epochs
        self.batch_size = int(5e3) # Number of rays per batch => Multiply by Nc for the number of samples
        self.hidden_dim = hidden_dim
        self.sampler = CoarseNeRFSampler(t_min=0, t_max=8, Nc=self.Nc) if sampler is None else sampler
        self.name = name

        self.branch1 = nn.Sequential(
            nn.Linear(6*self.Lx, hidden_dim), # 3 * (2 * Lx) because we have 3 values for x and 2*Lx for the positional encoding
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        self.branch2 = nn.Sequential(
            nn.Linear(hidden_dim+6*self.Lx, hidden_dim), # Concatenate the output of the first branch with gamma(x)
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            # nn.BatchNorm1d(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim + 1), # 1 for sigma
        )

        self.branch3 = nn.Sequential(
            nn.Linear(hidden_dim + 6*self.Ld, 128), # 6 for the positional encoding of d
            nn.ReLU(),
            # nn.BatchNorm1d(128),
            nn.Linear(128, 128),
            nn.ReLU(),
            # nn.BatchNorm1d(128),
            nn.Linear(128, 3), # 3 for the RGB values
        )

        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)        

    def forward(self, x, d):
        """
        Forward pass of the network
        @param x: Tensor of shape (n, 6 * Lx) containing the encoded x values of the sampled points
        @param d: Tensor of shape (n, 6 * Ld) containing the encoded d values of the sampled points
        """
        # First 5 layers of the network => Then we concatenate the output with gamma(x) as in figure 7 of the paper
        x_branch1 = self.branch1(x)
        concatenated_input = torch.cat((x_branch1, x), dim=1)

        # Next 4 layers of the network => Outputs sigma values for the Nc samples
        # Then we concatenate the output with gamma(d)
        x_branch2 = self.branch2(concatenated_input)
        concatenated_input = torch.cat((x_branch2[:, 1:], d), dim=1) # We remove the first element of x_branch2 because it is the sigma value

        sigma = x_branch2[:, :1]               # Sigma values
        rgb = self.branch3(concatenated_input) # RGB values

        concatenated_output = torch.cat((sigma, rgb), dim=1)
        return concatenated_output # Output: (σRGB)


    def encod_forward(self, x, d):
        """
        @param x: n_rays, Nc, 3
        @param d: n_rays, 3
        @output: n_rays, Nc, 4; σRGB values for each sample for each ray
        """
        n_rays = x.shape[0]

        d = d[:, None, :].expand(-1, self.Nc, -1).reshape(-1, 3)
        x = x.reshape(-1, 3)

        encoded_x = self.positional_encoding(x/8, self.Lx)
        encoded_d = self.positional_encoding(d, self.Ld)

        outputs = self(encoded_x, encoded_d)
        outputs = outputs.reshape(n_rays, self.Nc, 4)

        return outputs
    
    def output_to_color(self, output, t):
        """
        @param output: Tensor of shape (n_rays, Nc, 4) containing the output of the network
                       | Also works with output (n_rays * Nc, 4)
        @param t: Tensor of shape (n_rays, Nc+1)
        @ouput: Ci, Colors of each ray, shape (n_rays, 3)
        """
        output = output.reshape(-1, self.Nc, 4)
        
        sigma = output[:, :, 0]
        rgb = output[:, :, 1:]

        delta = t[:, 1:] - t[:, :-1] # shape: [n_rays, Nc]

        Ti = torch.exp(- torch.cumsum(sigma*delta, dim=1)) # Transmittance; shape = [n_rays, Nc]
        Ci = torch.sum(rgb * ((1 - torch.exp(- sigma * delta)) * Ti)[:, :, None], dim=1)
        return Ci

    def get_colors_from_ray(self, o, d, t):
        """
        @param o: Tensor of shape (n_rays, 3)
        @param d: Tensor of shape (n_rays, 3)
        @param t: Tensor of shape (n_rays, Nc + 1)
        @return ray_colors: Tensor of shape (n_rays, 3)
        """
        x = o[:, None, :] + d[:, None, :] * t[:, :-1, None] # shape: [n_rays, Nc, 3]
        ouputs = self.encod_forward(x, d) # shape: [n_rays, Nc, 3]
        ray_colors = self.output_to_color(ouputs, t)
        return ray_colors
    
    def sigma_from_ray(self, o, d, t):
        """
        @param o: Tensor of shape (n_rays, 3)
        @param d: Tensor of shape (n_rays, 3)
        @param t: Tensor of shape (n_rays, Nc + 1)
        @return sigmas: Tensor of shape (n_rays, Nc)
        """
        x = o[:, None, :] + d[:, None, :] * t[:, :-1, None]
        ouputs = self.encod_forward(x, d)
        return ouputs[:, :, 0]
    
    def sample_and_get_ray_color(self, o, d):
        """
        @param o: Tensor of shape (n_rays, 3)
        @param d: Tensor of shape (n_rays, 3)
        """
        t = self.sampler.sample(o, d)
        assert t.shape[1] == self.Nc + 1, f"Expected {self.Nc + 1} samples, got {t.shape[1]}"
        return self.get_colors_from_ray(o, d, t)

    def extract_rays_from_poses(self, poses):
        """
        @param poses: Tensor of shape (B, 4, 4), in the dataset
        """

        # Generate the camera pose for each pixel of a 100x100 image
        u, v = torch.meshgrid(torch.linspace(-image_height/2, image_height/2, image_height), torch.linspace(-image_width/2, image_width/2, image_width))
        us = u.to(device)
        vs = v.to(device)

        # Generate the origin and direction for each ray
        tensor_od = extract_origin_and_direction(poses, us, vs, camera_instrinsics) # shape: (im_height, im_width, B, 6)
        tensor_od = tensor_od.permute(2, 0, 1, 3) # shape: (B, im_height, im_width, 6)
                                                  # >> imgs_tensor.shape => (B, im_height, im_width, 3)
        return tensor_od
    
    def predict_scenes(self, poses):
        """
        @param poses: Tensor of shape (B, 4, 4), in the dataset
        """
        self.eval()
        with torch.no_grad():
            B = len(poses)
            poses = torch.as_tensor(poses, device=device)

            # We extract the rays from the pose
            tensor_od = self.extract_rays_from_poses(poses).reshape(-1, 6)
            o = tensor_od[:, :3]
            d = tensor_od[:, 3:]
            
            n_rays = o.shape[0]
            
            rgb_colors = []
            for i in range(0, n_rays, self.batch_size):
                os, ds = o[i:i+self.batch_size], d[i:i+self.batch_size]
                rgb_colors.append(
                    self.sample_and_get_ray_color(os, ds)
                )
            rgb_colors = torch.cat(rgb_colors, dim=0)

            rgb_colors = rgb_colors.reshape(B, image_height, image_width, 3)
        return rgb_colors

    def predict_scene(self, pose):
        pose = torch.as_tensor(pose, device=device, dtype=dtype)
        return self.predict_scenes(pose.unsqueeze(0))[0]
 

    def train_model(self, poses, imgs_tensor, validation_split=0.2, save=True):
        print('Preprocessing ...')
        torch.cuda.empty_cache()

        # === 3D HANDLING ===

        poses = torch.Tensor(poses).to(device)
        imgs_tensor = torch.Tensor(imgs_tensor).type(torch.DoubleTensor).to(device)

        # Generate the origin and direction for each ray
        tensor_od = self.extract_rays_from_poses(poses)

        # "Flatten" the tensors
        tensor_od = tensor_od.reshape(-1, tensor_od.shape[-1])          # origin and direction for each ray; shape => n_pixels * 6
        pixels_tensor = imgs_tensor.reshape(-1, imgs_tensor.shape[-1])  # rgb color associated to each ray; shape => n_pixels * 3
        del imgs_tensor

        # === DATASETS CREATION ===
        dataset = TensorDataset(tensor_od, pixels_tensor)

        train_size = int((1 - validation_split) * len(dataset))
        val_size = len(dataset) - train_size

        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size]) # Split the dataset into train and validation
        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True) # Make the data loaders
        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)

        print('Training the model ...')

        # === MODEL TRAINING ===

        self.train()
        for epoch in range(self.epochs):
            total_loss = 0
            for i, (od, target_rgb) in enumerate(train_dataloader):
                self.optimizer.zero_grad()
                
                # === CREATE SAMPLES AND ENCODE ===
                n_rays = od.shape[0]

                # Ray parameters
                o = od[:, :3]
                d = od[:, 3:]

                pred_rgb = self.sample_and_get_ray_color(o, d)

                loss = F.mse_loss(pred_rgb, target_rgb)
                loss = torch.mean((torch.log1p(pred_rgb) - torch.log1p(target_rgb)) ** 2)
                loss.backward()
                # print(loss.item())
                self.optimizer.step()
                total_loss += loss.item()

                if i % 100 == 0:
                    print(f"Epoch {epoch+1}/{self.epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item()}")
            print(f"Epoch {epoch+1}/{self.epochs}, Total Loss: {total_loss}")
            if epoch % 5 == 0 and save:
                print(f"Saving model at epoch {epoch}")
                torch.save(self.state_dict(), f"models/model_{self.name}_{epoch}.pt")


    def predict(self, x, d):
            x = self.positional_encoding(x/8, self.Lx)
            d = self.positional_encoding(d, self.Ld)
            self.eval()
            with torch.no_grad():
                  outputs = self(x, d)
            return outputs
    
    def positional_encoding(self, inputs, L = 6):
        # L = 6 is for position
        # L = 4 for direction

        # inputs = torch.tensor(inputs, device=device, dtype=dtype)
        freqs = 2.0 ** torch.arange(L, device=device, dtype=dtype)
        inputs_expanded = inputs.unsqueeze(-1)  # Shape: [batch_size, D, 1]
        inputs_freq = inputs_expanded * freqs   # Shape: [batch_size, D, L]

        # Apply sinusoidal functions
        sin_inputs = torch.sin(inputs_freq)
        cos_inputs = torch.cos(inputs_freq)

        # Reshape and concatenate
        sin_inputs = sin_inputs.reshape(inputs.shape[0], -1)
        cos_inputs = cos_inputs.reshape(inputs.shape[0], -1)
        periodic_fns = torch.cat([sin_inputs, cos_inputs], dim=-1)
        output = torch.cat([inputs, periodic_fns], dim=-1)
        return periodic_fns

    def get_color_prediction(self, tensor_od):
        """
        Takes as argument:
        * tensor_od: Tensor of shape (im_height, im_width, B, 6) containing the origin and direction of each ray
        Returns:
        * Tensor of shape (im_height * im_width * B, 3) containing the color prediction for each pixel
        """

        tensor_od = tensor_od.reshape(-1, 6) # Shape => (n_rays, 6)

        t = self.sampler.sample(tensor_od[:, :3], tensor_od[:, 3:]) # Sample points along the ray; shape: (n_rays, Nc+1)

        o, d = tensor_od[..., :3], tensor_od[..., 3:] # Split the tensor into x and d

        x = o[:, None, :] + d[:, None, :] * t[:,:-1][:, :, None] # Get the x values; shape: (n_rays, Nc, 3)

        x = x.reshape(-1, 3)

        x = self.positional_encoding(x/8, self.Lx) # Encode the x values
        d = self.positional_encoding(d, self.Ld) # Encode the d values
        d = d.repeat(self.Nc, 1)

        # outputs = self(x, d) # Get the outputs from the model
        
        # We use batch processing to avoid memory issues
        outputs = []
        for i in range(0, x.shape[0], self.batch_size):
            outputs.append(self(x[i:i+self.batch_size], d[i:i+self.batch_size]))
        outputs = torch.cat(outputs, dim=0)
        
        return self.output_to_color(outputs, t) # Get the color prediction
    

    def predict_image(self, pose):
        self.eval()
        with torch.no_grad():
            pose = torch.tensor(pose, device=device, dtype=dtype).unsqueeze(0)

            # Generate the origin and direction for each ray
            u, v = torch.meshgrid(torch.linspace(-image_height / 2, image_height / 2, image_height), torch.linspace(-image_width / 2, image_width / 2, image_width))
            tensor_od = extract_origin_and_direction(pose, u, v, camera_instrinsics) # shape: (im_height, im_width, B, 6)

            return self.get_color_prediction(tensor_od).reshape(image_height, image_width, 3)
